set.seed(1)
# Generate data
n <- 1000  # number of observations
p <- 20    # number of features
# True coefficients with some exact zeros
beta <- rep(0, p)
beta[1:5] <- rnorm(5)
# Generate predictors
X <- matrix(rnorm(n * p), nrow = n, ncol = p)
# Generate response
Y <- X %*% beta + rnorm(n)
set.seed(1)
# Generate data
n <- 1000  # number of observations
p <- 20    # number of features
# True coefficients with some exact zeros
beta <- rep(0, p)
beta[1:5] <- rnorm(5)
# Generate predictors
X <- matrix(rnorm(n * p), nrow = n, ncol = p)
# Generate response
Y <- X %*% beta + rnorm(n)
set.seed(1)
# Generate data
n <- 1000  # number of observations
p <- 20    # number of features
# True coefficients with some exact zeros
beta <- rep(0, p)
beta[1:5] <- rnorm(5)
# Generate predictors
X <- matrix(rnorm(n * p), nrow = n, ncol = p)
# Generate response
Y <- X %*% beta + rnorm(n)
library(leaps)
regfit.full <- regsubsets(X_train, Y_train, nvmax=p)
train_mse <- summary(regfit.full)$rss / nrow(X_train)
plot(1:p, train_mse, type="b", xlab="Number of Variables", ylab="Training MSE")
set.seed(1)
# Generate data
n <- 1000  # number of observations
p <- 20    # number of features
# True coefficients with some exact zeros
beta <- rep(0, p)
beta[1:5] <- rnorm(5)
# Generate predictors
X <- matrix(rnorm(n * p), nrow = n, ncol = p)
# Generate response
Y <- X %*% beta + rnorm(n)
set.seed(1)
# Generate data
n <- 1000  # number of observations
p <- 20    # number of features
# True coefficients with some exact zeros
beta <- rep(0, p)
beta[1:5] <- rnorm(5)
# Generate predictors
X <- matrix(rnorm(n * p), nrow = n, ncol = p)
# Generate response
Y <- X %*% beta + rnorm(n)
set.seed(1)
# Generate data
n <- 1000  # number of observations
p <- 20    # number of features
# True coefficients with some exact zeros
beta <- rep(0, p)
beta[1:5] <- rnorm(5)
# Generate predictors
X <- matrix(rnorm(n * p), nrow = n, ncol = p)
# Generate response
Y <- X %*% beta + rnorm(n)
# Load required libraries
library(ISLR)
library(leaps)
library(gam)
library(MASS)
# Load the data
data(College)
# (a) Split the data and perform forward stepwise selection
# Split the data (80% training, 20% test)
set.seed(123)
train_index <- sample(1:nrow(College), 0.8 * nrow(College))
train_data <- College[train_index, ]
test_data <- College[-train_index, ]
# Perform forward stepwise selection
forward_model <- regsubsets(Outstate ~ ., data = train_data, method = "forward", nvmax = 17)
summary_forward <- summary(forward_model)
# Choose the best model based on adjusted R-squared
best_model_size <- which.max(summary_forward$adjr2)
best_predictors <- names(coef(forward_model, best_model_size))[-1]  # Exclude intercept
cat("Selected predictors:", paste(best_predictors, collapse=", "), "\n")
# (b) Fit the GAM on the training data
gam.fit <- gam(Outstate ~ Private +
s(Room.Board, df = 2) +
s(PhD, df = 2) +
s(perc.alumni, df = 2) +
s(Expend, df = 5) +
s(Grad.Rate, df = 2),
data = train_data)
# Plot the results
par(mfrow = c(2, 3))
plot(gam.fit, se = TRUE, col = "blue")
# Print summary of the GAM model
print(summary(gam.fit))
# (c) Evaluate the model on the test set
# Make predictions on the test set
predictions <- predict(gam.fit, newdata = test_data)
# Calculate MSE
mse <- mean((test_data$Outstate - predictions)^2)
cat("Mean Squared Error on test set:", mse, "\n")
# Calculate R-squared
rsq <- 1 - sum((test_data$Outstate - predictions)^2) / sum((test_data$Outstate - mean(test_data$Outstate))^2)
cat("R-squared on test set:", rsq, "\n")
# Set seed for reproducibility
set.seed(123)
# (a) Generate data
n <- 100
X1 <- rnorm(n)
X2 <- rnorm(n)
Y <- 2 + 3*X1 + 4*X2 + rnorm(n)
# (b) Initialize beta1
beta1 <- 0  # You can choose any initial value
iterations <- 1000
# (e) Perform backfitting
iterations <- 1000
beta0_values <- numeric(iterations)
beta1_values <- numeric(iterations)
beta2_values <- numeric(iterations)
for(i in 1:iterations) {
# (c) Update beta2
a <- Y - beta1 * X1
fit2 <- lm(a ~ X2)
beta0 <- fit2$coef[1]
beta2 <- fit2$coef[2]
# (d) Update beta1
a <- Y - beta2 * X2
fit1 <- lm(a ~ X1)
beta0 <- fit1$coef[1]
beta1 <- fit1$coef[2]
# Store values
beta0_values[i] <- beta0
beta1_values[i] <- beta1
beta2_values[i] <- beta2
}
# Plot results
plot(1:iterations, beta0_values, type="l", col="red", ylim=range(c(beta0_values, beta1_values, beta2_values)),
xlab="Iteration", ylab="Coefficient Estimate")
lines(1:iterations, beta1_values, col="green")
lines(1:iterations, beta2_values, col="blue")
legend("topright", legend=c("beta0", "beta1", "beta2"), col=c("red", "green", "blue"), lty=1)
# (f) Compare with multiple linear regression
mlr_fit <- lm(Y ~ X1 + X2)
# Print final backfitting estimates and multiple regression estimates
cat("Final backfitting estimates:\n")
cat("beta0:", beta0_values[iterations], "\n")
cat("beta1:", beta1_values[iterations], "\n")
cat("beta2:", beta2_values[iterations], "\n")
cat("\nMultiple regression estimates:\n")
print(mlr_fit$coefficients)
# Plotting
plot(Y ~ X1 + X2)
# Add horizontal lines for coefficients
abline(h = coef(mlr_fit)[1], col = "red", lty = 2)
abline(h = coef(mlr_fit)[2], col = "green", lty = 2)
abline(h = coef(mlr_fit)[3], col = "blue", lty = 2)
# (g) Determine number of iterations for good approximation
tolerance <- 0.001
good_approx <- which(abs(beta0_values - mlr_fit$coefficients[1]) < tolerance &
abs(beta1_values - mlr_fit$coefficients[2]) < tolerance &
abs(beta2_values - mlr_fit$coefficients[3]) < tolerance)[1]
cat("\nNumber of iterations for good approximation:", good_approx)
# (g) Determine number of iterations for good approximation
tolerance <- 0.001
good_approx <- which(abs(beta0_values - mlr_fit$coefficients[1]) < tolerance &
abs(beta1_values - mlr_fit$coefficients[2]) < tolerance &
abs(beta2_values - mlr_fit$coefficients[3]) < tolerance)[1]
cat("\nNumber of iterations for good approximation:", good_approx)
load("D:/Desktop/CSP-571-Data-Preparation-and-Analysis-Project-Analyzing-Divvy-Bike-Usage-Patterns-in-Chicago-/R Implementation/Data Cleaning and Data Transformation.Rmd")
setwd("~/")
setwd("~/")
if ("xfun" %in% loadedNamespaces()) {
detach("package:xfun", unload = TRUE)
}
remove.packages("xfun")
install.packages(c("cli", "clock", "digest", "gam", "ipred", "Rcpp", "rlang", "SparseM", "tinytex", "xfun"), type = "source")
if ("xfun" %in% loadedNamespaces()) {
detach("package:xfun", unload = TRUE)
}
remove.packages("xfun")
install.packages(c("cli", "clock", "digest", "gam", "ipred", "Rcpp", "rlang", "SparseM", "tinytex", "xfun"), type = "source")
# Train Control
train_control <- trainControl(method = "cv", number = 10)
# Sample a smaller subset for training
set.seed(123)
train_sample_size <- 1000  # Further reduce sample size if necessary
train_sampled_data <- selected_data[sample(.N, train_sample_size), ]
# Load necessary libraries
library(caret)
library(randomForest)
library(data.table)
# Convert to data.table for efficiency
transformed_data <- as.data.table(transformed_data)
# Load the transformed data
transformed_data <- read.csv("transformed_data.csv")
# Load necessary libraries
library(caret)
library(randomForest)
library(data.table)
# Convert to data.table for efficiency
transformed_data <- as.data.table(transformed_data)
# Correlation Analysis
cor_matrix <- cor(transformed_data %>% select_if(is.numeric), use = "complete.obs")
knitr::opts_chunk$set(echo = TRUE)
library(ISLR)
library(neuralnet)
library(keras)
library(tensorflow)
library(caret)
library(ggplot2)
library(e1071)
# Load the Default dataset
data(Default)
# Encode the categorical variables
Default$default <- ifelse(Default$default == "Yes", 1, 0)
Default$student <- ifelse(Default$student == "Yes", 1, 0)
# Split the data into training and testing sets
set.seed(123)
train_indices <- sample(1:nrow(Default), size = 0.7 * nrow(Default))
train_data <- Default[train_indices, ]
test_data <- Default[-train_indices, ]
# Separate features and labels
train_x <- as.matrix(train_data[, c("balance", "income", "student")])
train_y <- as.matrix(train_data$default)
test_x <- as.matrix(test_data[, c("balance", "income", "student")])
test_y <- as.matrix(test_data$default)
# Define the neural network model
model <- keras_model_sequential() %>%
layer_dense(units = 10, activation = 'relu', input_shape = c(3)) %>%
layer_dropout(rate = 0.5) %>%
layer_dense(units = 1, activation = 'sigmoid')
knitr::opts_chunk$set(echo = TRUE)
install.packages("ggplot2")
install.packages("dendextend")
install.packages("ggplot2")
library(ggplot2)
library(dendextend)
# Load the USArrests data
data("USArrests")
# Perform hierarchical clustering with complete linkage and Euclidean distance
d <- dist(USArrests, method = "euclidean")
hc_complete <- hclust(d, method = "complete")
# Plot the dendrogram
plot(hc_complete, main = "Hierarchical Clustering with Complete Linkage", xlab = "", sub = "", cex = 0.9)
# Cut the dendrogram to obtain three clusters
clusters <- cutree(hc_complete, k = 3)
# Print the clusters
clusters
# Show the clusters on the dendrogram
dend <- as.dendrogram(hc_complete)
dend <- color_branches(dend, k = 3)
plot(dend, main = "Dendrogram with Three Clusters Highlighted")
# Scale the variables
USArrests_scaled <- scale(USArrests)
# Perform hierarchical clustering with scaled variables
d_scaled <- dist(USArrests_scaled, method = "euclidean")
hc_complete_scaled <- hclust(d_scaled, method = "complete")
# Plot the dendrogram with scaled variables
plot(hc_complete_scaled, main = "Hierarchical Clustering with Scaled Variables", xlab = "", sub = "", cex = 0.9)
# Set seed for reproducibility
set.seed(123)
# Generate data for three classes
class1 <- matrix(rnorm(20 * 50, mean = 0), nrow = 20, ncol = 50)
class2 <- matrix(rnorm(20 * 50, mean = 3), nrow = 20, ncol = 50)
class3 <- matrix(rnorm(20 * 50, mean = 6), nrow = 20, ncol = 50)
# Combine the data into one dataset
data <- rbind(class1, class2, class3)
class_labels <- c(rep(1, 20), rep(2, 20), rep(3, 20))
# Convert to data frame for easier handling
data_df <- data.frame(data)
data_df$class <- as.factor(class_labels)
# Perform PCA
pca_result <- prcomp(data, scale. = TRUE)
# Get the first two principal components
pc_data <- data.frame(PC1 = pca_result$x[, 1], PC2 = pca_result$x[, 2], Class = as.factor(class_labels))
# Plot the first two principal components
library(ggplot2)
ggplot(pc_data, aes(x = PC1, y = PC2, color = Class)) +
geom_point(size = 3) +
labs(title = "PCA - First Two Principal Components") +
theme_minimal()
# Perform K-means clustering with K = 3
set.seed(123)
kmeans_result <- kmeans(data, centers = 3, nstart = 20)
# Compare the clusters with the true class labels
table(True = class_labels, Cluster = kmeans_result$cluster)
# Perform K-means clustering with K = 2
set.seed(123)
kmeans_result_k2 <- kmeans(data, centers = 2, nstart = 20)
# Compare the clusters with the true class labels
table(True = class_labels, Cluster = kmeans_result_k2$cluster)
# Perform K-means clustering with K = 4
set.seed(123)
kmeans_result_k4 <- kmeans(data, centers = 4, nstart = 20)
# Compare the clusters with the true class labels
table(True = class_labels, Cluster = kmeans_result_k4$cluster)
# Perform K-means clustering on the first two principal components with K = 3
set.seed(123)
kmeans_pc_result <- kmeans(pc_data[, 1:2], centers = 3, nstart = 20)
# Compare the clusters with the true class labels
table(True = class_labels, Cluster = kmeans_pc_result$cluster)
# Scale the data
data_scaled <- scale(data)
# Perform K-means clustering with K = 3 on scaled data
set.seed(123)
kmeans_scaled_result <- kmeans(data_scaled, centers = 3, nstart = 20)
# Compare the clusters with the true class labels
table(True = class_labels, Cluster = kmeans_scaled_result$cluster)
library(ISLR)
library(neuralnet)
library(keras)
library(tensorflow)
library(caret)
library(ggplot2)
# Load the Default dataset
data(Default)
# Encode the categorical variables
Default$default <- ifelse(Default$default == "Yes", 1, 0)
Default$student <- ifelse(Default$student == "Yes", 1, 0)
# Split the data into training and testing sets
set.seed(123)
train_indices <- sample(1:nrow(Default), size = 0.7 * nrow(Default))
train_data <- Default[train_indices, ]
test_data <- Default[-train_indices, ]
# Separate features and labels
train_x <- as.matrix(train_data[, c("balance", "income", "student")])
train_y <- as.matrix(train_data$default)
test_x <- as.matrix(test_data[, c("balance", "income", "student")])
test_y <- as.matrix(test_data$default)
# Define the neural network model
model <- keras_model_sequential() %>%
layer_dense(units = 10, activation = 'relu', input_shape = c(3)) %>%
layer_dropout(rate = 0.5) %>%
layer_dense(units = 1, activation = 'sigmoid')
library(ISLR)
library(neuralnet)
library(caret)
library(ggplot2)
# Load the Default dataset
data(Default)
# Encode the categorical variables
Default$default <- ifelse(Default$default == "Yes", 1, 0)
Default$student <- ifelse(Default$student == "Yes", 1, 0)
# Split the data into training and testing sets
set.seed(123)
train_indices <- sample(1:nrow(Default), size = 0.7 * nrow(Default))
train_data <- Default[train_indices, ]
test_data <- Default[-train_indices, ]
# Separate features and labels
train_x <- train_data[, c("balance", "income", "student")]
train_y <- train_data$default
test_x <- test_data[, c("balance", "income", "student")]
test_y <- test_data$default
# Encode the categorical variables
Default$default <- ifelse(Default$default == "Yes", 1, 0)
Default$student <- ifelse(Default$student == "Yes", 1, 0)
# Split the data into training and testing sets
set.seed(123)
train_indices <- sample(1:nrow(Default), size = 0.7 * nrow(Default))
train_data <- Default[train_indices, ]
test_data <- Default[-train_indices, ]
# Separate features and labels
train_x <- train_data[, c("balance", "income", "student")]
train_y <- train_data$default
test_x <- test_data[, c("balance", "income", "student")]
test_y <- test_data$default
# Define the formula for the neural network
nn_formula <- as.formula("default ~ balance + income + student")
# Train the neural network model
nn_model <- neuralnet(nn_formula, data = train_data, hidden = 10, linear.output = FALSE)
# Plot the neural network
plot(nn_model)
# Predict on the testing set using the neural network
nn_pred <- compute(nn_model, test_x)$net.result
nn_pred <- ifelse(nn_pred > 0.5, 1, 0)
# Confusion matrix for neural network
nn_conf_matrix <- confusionMatrix(as.factor(nn_pred), as.factor(test_y))
# Encode the categorical variables
Default$default <- ifelse(Default$default == "Yes", 1, 0)
Default$student <- ifelse(Default$student == "Yes", 1, 0)
# Split the data into training and testing sets
set.seed(123)
train_indices <- sample(1:nrow(Default), size = 0.7 * nrow(Default))
train_data <- Default[train_indices, ]
test_data <- Default[-train_indices, ]
# Separate features and labels
train_x <- train_data[, c("balance", "income", "student")]
train_y <- train_data$default
test_x <- test_data[, c("balance", "income", "student")]
test_y <- test_data$default
# Define the formula for the neural network
nn_formula <- as.formula("default ~ balance + income + student")
# Train the neural network model
nn_model <- neuralnet(nn_formula, data = train_data, hidden = 10, linear.output = FALSE)
# Plot the neural network
plot(nn_model)
# Predict on the testing set using the neural network
nn_pred <- compute(nn_model, test_x)$net.result
nn_pred <- ifelse(nn_pred > 0.5, 1, 0)
# Ensure that nn_pred has at least two levels
if(length(unique(nn_pred)) == 1) {
nn_pred[1] <- 1 - nn_pred[1]
}
# Confusion matrix for neural network
nn_conf_matrix <- confusionMatrix(as.factor(nn_pred), as.factor(test_y))
# Encode the categorical variables
Default$default <- ifelse(Default$default == "Yes", 1, 0)
Default$student <- ifelse(Default$student == "Yes", 1, 0)
# Split the data into training and testing sets
set.seed(123)
train_indices <- sample(1:nrow(Default), size = 0.7 * nrow(Default))
train_data <- Default[train_indices, ]
test_data <- Default[-train_indices, ]
# Separate features and labels
train_x <- train_data[, c("balance", "income", "student")]
train_y <- train_data$default
test_x <- test_data[, c("balance", "income", "student")]
test_y <- test_data$default
# Define the formula for the neural network
nn_formula <- as.formula("default ~ balance + income + student")
# Train the neural network model
nn_model <- neuralnet(nn_formula, data = train_data, hidden = 10, linear.output = FALSE)
# Plot the neural network
plot(nn_model)
# Predict on the testing set using the neural network
nn_pred <- compute(nn_model, test_x)$net.result
nn_pred <- ifelse(nn_pred > 0.5, 1, 0)
# Ensure that nn_pred has the same levels as test_y
nn_pred <- factor(nn_pred, levels = c(0, 1))
test_y <- factor(test_y, levels = c(0, 1))
# Confusion matrix for neural network
nn_conf_matrix <- confusionMatrix(nn_pred, test_y)
print(nn_conf_matrix)
# Plot confusion matrix for neural network
nn_conf_matrix_plot <- as.data.frame(table(Prediction = nn_pred, Reference = test_y))
ggplot(data = nn_conf_matrix_plot, aes(x = Reference, y = Prediction, fill = Freq)) +
geom_tile() +
geom_text(aes(label = Freq)) +
scale_fill_gradient(low = "white", high = "blue") +
labs(title = "Confusion Matrix for Neural Network", x = "Actual", y = "Predicted")
# Fit a logistic regression model
logistic_model <- glm(default ~ balance + income + student, data = train_data, family = binomial)
# Predict on the testing set using logistic regression
logistic_pred_prob <- predict(logistic_model, test_data, type = "response")
logistic_pred <- ifelse(logistic_pred_prob > 0.5, 1, 0)
# Ensure that logistic_pred has the same levels as test_y
logistic_pred <- factor(logistic_pred, levels = c(0, 1))
# Evaluate the logistic regression model
logistic_accuracy <- mean(logistic_pred == test_y)
logistic_accuracy
# Confusion matrix for logistic regression
logistic_conf_matrix <- confusionMatrix(logistic_pred, test_y)
print(logistic_conf_matrix)
# Plot confusion matrix for logistic regression
logistic_conf_matrix_plot <- as.data.frame(table(Prediction = logistic_pred, Reference = test_y))
ggplot(data = logistic_conf_matrix_plot, aes(x = Reference, y = Prediction, fill = Freq)) +
geom_tile() +
geom_text(aes(label = Freq)) +
scale_fill_gradient(low = "white", high = "blue") +
labs(title = "Confusion Matrix for Logistic Regression", x = "Actual", y = "Predicted")
# Print the evaluation results
cat("Neural Network Accuracy:", nn_conf_matrix$overall['Accuracy'], "\n")
cat("Logistic Regression Accuracy:", logistic_accuracy, "\n")
knitr::opts_chunk$set(echo = TRUE)
# Load the transformed data
transformed_data <- read.csv("transformed_data.csv")
# Load necessary libraries
library(ggplot2)
library(dplyr)
# Load the transformed data
#transformed_data <- read.csv("C:/Users/usman/OneDrive - Illinois Institute of #Technology/DPA/Project/Divvy data/transformed_data.csv")
# Violin Plot for Member Type vs Trip Duration
ggplot(transformed_data, aes(x = member_casual, y = trip_duration)) +
geom_violin(fill = "steelblue", alpha = 0.7) +
labs(
title = "Violin Plot of Trip Duration by Member Type",
x = "Member Type",
y = "Trip Duration (minutes)"
) +
theme_minimal()
# Load necessary libraries
library(caret)
library(randomForest)
library(data.table)
# Convert to data.table for efficiency
transformed_data <- as.data.table(transformed_data)
# Correlation Analysis
cor_matrix <- cor(transformed_data %>% select_if(is.numeric), use = "complete.obs")
highly_correlated <- findCorrelation(cor_matrix, cutoff = 0.75)
correlated_features <- colnames(transformed_data)[highly_correlated]
# Remove highly correlated features
selected_features <- transformed_data %>% select(-one_of(correlated_features))
# Sample a subset for RFE to reduce memory usage
set.seed(123)
sample_size <- 10000  # Adjust sample size as necessary
sampled_data <- selected_features[sample(.N, sample_size), ]
# Ensure the response variable is included in the sampled data
sampled_data$trip_duration <- transformed_data[sample(.N, sample_size), trip_duration]
# Recursive Feature Elimination (RFE)
control <- rfeControl(functions = rfFuncs, method = "cv", number = 10)
rfe_result <- rfe(sampled_data %>% select(-trip_duration),
sampled_data$trip_duration,
sizes = c(1:10),
rfeControl = control)
