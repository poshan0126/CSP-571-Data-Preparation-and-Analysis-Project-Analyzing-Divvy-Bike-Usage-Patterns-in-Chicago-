---
title: "Data analysis and modeling"
author: "Usman Matheen"
date: "2024-07-10"
output: html_document
---


### Distribution Analysis


```{r}
library(googledrive)
library(dplyr)

# Load the transformed data
transformed_data <- read.csv("transformed_data.csv")

# Use the transformed_data as needed

```

```{r}
# Load necessary libraries
library(ggplot2)
library(dplyr)

# Load the transformed data
#transformed_data <- read.csv("C:/Users/usman/OneDrive - Illinois Institute of #Technology/DPA/Project/Divvy data/transformed_data.csv")

# Violin Plot for Member Type vs Trip Duration
ggplot(transformed_data, aes(x = member_casual, y = trip_duration)) +
  geom_violin(fill = "steelblue", alpha = 0.7) +
  labs(
    title = "Violin Plot of Trip Duration by Member Type",
    x = "Member Type",
    y = "Trip Duration (minutes)"
  ) +
  theme_minimal()
```

```{r}
# Rug Plot of Trip Duration
ggplot(transformed_data, aes(x = trip_duration)) +
  geom_rug(sides = "b", color = "steelblue") +
  labs(
    title = "Rug Plot of Trip Duration",
    x = "Trip Duration (minutes)",
    y = "Density"
  ) +
  theme_minimal()
```


```{r}
library(GGally)
numeric_columns <- transformed_data %>%
  select(trip_duration, start_hour) %>%
  na.omit()

ggpairs(numeric_columns) +
  labs(title = "Scatterplot Matrix for Time Duration") +
  theme_minimal()
```


```{r}
# Member vs Casual Riders by Ride Type for Each Month
ggplot(transformed_data, aes(x = start_month, fill = member_casual)) +
  geom_bar(position = "dodge") +
  labs(
    title = "Member vs Casual Riders by Ride Type for Each Month",
    x = "Month",
    y = "Count",
    fill = "Member Type"
  ) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```
```{r}
# Member vs Casual Riders by Seasons
ggplot(transformed_data, aes(x = season, fill = member_casual)) +
  geom_bar(position = "dodge") +
  labs(
    title = "Member vs Casual Riders by Seasons",
    x = "Season",
    y = "Count",
    fill = "Member Type"
  ) +
  theme_minimal()
```

```{r}
# Time of Day Distribution
ggplot(transformed_data, aes(x = time_of_day)) +
  geom_bar(fill = "steelblue") +
  labs(
    title = "Distribution of Trips by Time of Day",
    x = "Time of Day",
    y = "Count"
  ) +
  theme_minimal()
```

```{r}
# Starting Station Distribution (Top 10)
top_start_stations <- transformed_data %>%
  count(start_station_name, sort = TRUE) %>%
  top_n(10, wt = n)

ggplot(top_start_stations, aes(x = reorder(start_station_name, n), y = n)) +
  geom_bar(stat = "identity", fill = "steelblue") +
  coord_flip() +
  labs(
    title = "Top 10 Starting Stations",
    x = "Start Station",
    y = "Count"
  ) +
  theme_minimal()
```

```{r}
# End Station Distribution (Top 10)
top_end_stations <- transformed_data %>%
  count(end_station_name, sort = TRUE) %>%
  top_n(10, wt = n)

ggplot(top_end_stations, aes(x = reorder(end_station_name, n), y = n)) +
  geom_bar(stat = "identity", fill = "steelblue") +
  coord_flip() +
  labs(
    title = "Top 10 Ending Stations",
    x = "End Station",
    y = "Count"
  ) +
  theme_minimal()
```

### Clustering


```{r}
library(tidyr)
library(cluster)
library(factoextra)
```

```{r}
library(dplyr)
# Sample a subset of the data
set.seed(123)
sample_size <- 10000  # Adjust sample size based on your memory constraints
sampled_data <- transformed_data[sample(nrow(transformed_data), sample_size), ]

# Convert non-numeric columns to numeric using their underlying integer codes
clustering_data <- sampled_data %>%
  select(trip_duration, start_hour, day_of_week, season, time_of_day) %>%
  mutate(across(c(day_of_week, season, time_of_day), ~ as.numeric(as.factor(.))))

# Check for NA values after conversion
if (any(is.na(clustering_data))) {
  warning("There are NA values in the data after conversion. Please check your data.")
}

# Scale the data
clustering_data_scaled <- scale(clustering_data)

```

```{r}
# Elbow Method with sampled data
set.seed(123)
elbow_plot <- fviz_nbclust(clustering_data_scaled, kmeans, method = "wss") +
  labs(
    title = "Elbow Method for Optimal Number of Clusters (Sampled Data)",
    x = "Number of Clusters",
    y = "Total Within-Cluster Sum of Squares"
  ) +
  theme_minimal()
print(elbow_plot)
```


```{r}
# Silhouette Method with sampled data
set.seed(123)
silhouette_plot <- fviz_nbclust(clustering_data_scaled, kmeans, method = "silhouette") +
  labs(
    title = "Silhouette Method for Optimal Number of Clusters (Sampled Data)",
    x = "Number of Clusters",
    y = "Average Silhouette Width"
  ) +
  theme_minimal()
print(silhouette_plot)
```

```{r}
set.seed(123)
gap_stat <- clusGap(clustering_data_scaled, FUN = kmeans, nstart = 25, K.max = 10, B = 50)
gap_stat_plot <- fviz_gap_stat(gap_stat) +
  labs(
    title = "Gap Statistic Method for Optimal Number of Clusters (Sampled Data)",
    x = "Number of Clusters",
    y = "Gap Statistic"
  ) +
  theme_minimal()
print(gap_stat_plot)
```

```{r}
optimal_clusters <- 3
```

```{r}
set.seed(123)
kmeans_result <- kmeans(clustering_data_scaled, centers = optimal_clusters, nstart = 25)

# Add cluster assignments to the sampled data
sampled_data$cluster <- kmeans_result$cluster

# Evaluate clusters
wss <- sum(kmeans_result$withinss)
silhouette_score <- silhouette(sampled_data$cluster, dist(clustering_data_scaled))

# Print the evaluation metrics
cat("Total Within-Cluster Sum of Squares (WSS):", wss, "\n")
cat("Average Silhouette Width:", mean(silhouette_score[, 3]), "\n")
```

```{r}
# Visualize the clusters
fviz_cluster(kmeans_result, data = clustering_data_scaled) +
  labs(
    title = paste("K-means Clustering with", optimal_clusters, "Clusters"),
    x = "Feature 1",
    y = "Feature 2"
  ) +
  theme_minimal()
```
```{r}
# Visualize clusters using ggplot2
ggplot(sampled_data, aes(x = start_hour, y = trip_duration, color = as.factor(cluster))) +
  geom_point(alpha = 0.6) +
  labs(
    title = "Clusters of Trip Duration vs Start Hour",
    x = "Start Hour",
    y = "Trip Duration (minutes)",
    color = "Cluster"
  ) +
  theme_minimal()
```
```{r}
# Visualize clusters by day of the week
ggplot(sampled_data, aes(x = as.numeric(day_of_week), y = trip_duration, color = as.factor(cluster))) +
  geom_point(alpha = 0.6) +
  labs(
    title = "Clusters of Trip Duration vs Day of the Week",
    x = "Day of the Week",
    y = "Trip Duration (minutes)",
    color = "Cluster"
  ) +
  theme_minimal()

# Visualize clusters by season
ggplot(sampled_data, aes(x = season, y = trip_duration, color = as.factor(cluster))) +
  geom_point(alpha = 0.6) +
  labs(
    title = "Clusters of Trip Duration vs Season",
    x = "Season",
    y = "Trip Duration (minutes)",
    color = "Cluster"
  ) +
  theme_minimal()
```
```{r}
# Visualize clusters by time of day
ggplot(sampled_data, aes(x = time_of_day, y = trip_duration, color = as.factor(cluster))) +
  geom_point(alpha = 0.6) +
  labs(
    title = "Clusters of Trip Duration vs Time of Day",
    x = "Time of Day",
    y = "Trip Duration (minutes)",
    color = "Cluster"
  ) +
  theme_minimal()
```

```{r}
# Libraries for PCA and visualization
library(Rtsne)
library(ggplot2)
library(dplyr)

# Sample a subset of the data
set.seed(123)
sample_size <- 10000  # Adjust sample size based on your memory constraints
sampled_data <- transformed_data[sample(nrow(transformed_data), sample_size), ]

# Filter out rows with NA in member_casual
sampled_data <- sampled_data %>% filter(!is.na(member_casual))

# Select columns for dimensionality reduction
reduction_data <- sampled_data %>%
  select(
    trip_duration, 
    start_hour, 
    trip_duration_normalized, 
    trip_duration_standardized, 
    trip_duration_log, 
    start_day, 
    start_minute, 
    end_day, 
    end_minute, 
    trip_duration_hours
  )

# Remove duplicates
reduction_data <- reduction_data %>% distinct()

# Ensure row names are preserved
rownames(reduction_data) <- 1:nrow(reduction_data)

# Retain the corresponding rows in the original data
original_data <- sampled_data[rownames(reduction_data), ]

# Standardize the data
reduction_data_scaled <- scale(reduction_data)

# Perform PCA
pca_result <- prcomp(reduction_data_scaled, center = TRUE, scale. = TRUE)

# Extract PC scores
pca_scores <- as.data.frame(predict(pca_result))

# Set the column names of pca_scores to match the original features
colnames(pca_scores) <- colnames(reduction_data_scaled)

# Scree Plot
fviz_eig(pca_result)

# Contribution of variables
fviz_contrib(pca_result, choice = "var", axes = 1, top = 10) + labs(title = "Contribution of Variables to trip_duration")
fviz_pca_var(pca_result, col.var = "cos2", gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"), repel = TRUE) + labs(title = "Contribution of Variables")

# Individual PCA Plot
fviz_pca_ind(pca_result, geom.ind = "point", pointshape = 21, pointsize = 2, fill.ind = original_data$member_casual, 
             col.ind = "black", palette = "jco", addEllipses = TRUE, legend.title = "Group")

# Biplot
fviz_pca_biplot(pca_result,geom.ind = "point", pointshape = 21, pointsize = 2, fill.ind = original_data$member_casual,  
                col.ind = "black",palette = "jco", addEllipses = TRUE, label = "var",  col.var = "black", repel = TRUE,legend.title = "Group")


# Combined biplot with cos2
fviz_pca_biplot(pca_result, geom.ind = "point", pointshape = 21, pointsize = 2, fill.ind = original_data$member_casual, 
                col.ind = "black", palette = "jco", addEllipses = TRUE, label = "var", col.var = "cos2", 
                gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),repel = TRUE,legend.title = "Group")

# Additional PCA Plots
ggplot(pca_scores, aes(x = trip_duration_hours, y = start_hour)) +
  geom_point(aes(color = original_data$member_casual), alpha = 0.6) +
  labs(
    title = "PCA: trip_duration_hours vs start_hour",
    x = "trip_duration_hours",
    y = "start_hour"
  ) +
  theme_minimal()

ggplot(pca_scores, aes(x = start_day, y = end_day)) +
  geom_point(aes(color = original_data$member_casual), alpha = 0.6) +
  labs(
    title = "PCA: start_day vs end_day",
    x = "start_day",
    y = "end_day"
  ) +
  theme_minimal()

# Perform t-SNE
tsne_result <- Rtsne(reduction_data_scaled, dims = 2, perplexity = 30, verbose = TRUE)

# Extract t-SNE coordinates
tsne_coords <- as.data.frame(tsne_result$Y)

# Visualize t-SNE results
ggplot(tsne_coords, aes(x = V1, y = V2)) +
  geom_point(alpha = 0.6) +
  labs(
    title = "t-Distributed Stochastic Neighbor Embedding",
    x = "t-SNE 1",
    y = "t-SNE 2"
  ) +
  theme_minimal()
  # Feature Selection
```
  
```{r}
# Load necessary libraries
library(caret)
library(randomForest)
library(data.table)

# Convert to data.table for efficiency
transformed_data <- as.data.table(transformed_data)

# Correlation Analysis
cor_matrix <- cor(transformed_data %>% select_if(is.numeric), use = "complete.obs")
highly_correlated <- findCorrelation(cor_matrix, cutoff = 0.75)
correlated_features <- colnames(transformed_data)[highly_correlated]

# Remove highly correlated features
selected_features <- transformed_data %>% select(-one_of(correlated_features))

# Sample a subset for RFE to reduce memory usage
set.seed(123)
sample_size <- 10000  # Adjust sample size as necessary
sampled_data <- selected_features[sample(.N, sample_size), ]

# Ensure the response variable is included in the sampled data
sampled_data$trip_duration <- transformed_data[sample(.N, sample_size), trip_duration]

# Recursive Feature Elimination (RFE)
control <- rfeControl(functions = rfFuncs, method = "cv", number = 10)
rfe_result <- rfe(sampled_data %>% select(-trip_duration), 
                  sampled_data$trip_duration, 
                  sizes = c(1:10), 
                  rfeControl = control)

# Selected Features
rfe_selected_features <- predictors(rfe_result)
selected_data <- transformed_data %>% select(all_of(rfe_selected_features), trip_duration)

# Save selected features
write.csv(selected_data, "selected_data.csv", row.names = FALSE)
```

# Model Selection and Training
```{r}
# Ensure data.table is being used
selected_data <- as.data.table(selected_data)

# Convert date columns to numeric to avoid factor level issues
selected_data[, `:=`(
  start_date = as.numeric(as.POSIXct(start_date, origin = "1970-01-01")),
  end_date = as.numeric(as.POSIXct(end_date, origin = "1970-01-01"))
)]

# Convert character columns to factors
selected_data[, `:=`(
  start_month = factor(start_month),
  rideable_type = factor(rideable_type),
  end_month = factor(end_month),
  season = factor(season),
  member_casual = factor(member_casual),
  start_station_name = factor(start_station_name),
  time_of_day = factor(time_of_day)
)]

# Sample a smaller subset for training
set.seed(123)
train_sample_size <- 500  # Reduce sample size for faster computation
train_sampled_data <- selected_data[sample(1:nrow(selected_data), train_sample_size, replace = FALSE), ]

# Split the data into training and test sets
set.seed(123)
train_index <- createDataPartition(train_sampled_data$trip_duration, p = 0.8, list = FALSE)
train_data <- train_sampled_data[train_index, ]
test_data <- train_sampled_data[-train_index, ]

# Ensure consistent levels for categorical variables between train and test datasets
for (col in c("start_month", "rideable_type", "end_month", "season", "member_casual", "start_station_name", "time_of_day")) {
  test_data[[col]] <- factor(test_data[[col]], levels = levels(train_data[[col]]))
}
```

```{r}
# Train Control
train_control <- trainControl(method = "cv", number = 5)

# Random Forest
set.seed(123)
rf_model <- train(trip_duration ~ ., data = train_data, method = "rf", trControl = train_control, tuneLength = 3)

# Predict on the test set
rf_predictions <- predict(rf_model, test_data)

# Bin the trip_duration into categories (e.g., low, medium, high) for classification
# Define binning thresholds or use quantiles
breaks <- quantile(selected_data$trip_duration, probs = c(0, 0.33, 0.66, 1), na.rm = TRUE)
train_data$trip_duration_binned <- cut(train_data$trip_duration, breaks = breaks, labels = c("low", "medium", "high"), include.lowest = TRUE)
test_data$trip_duration_binned <- cut(test_data$trip_duration, breaks = breaks, labels = c("low", "medium", "high"), include.lowest = TRUE)

# Train Random Forest model on binned categories
rf_model_binned <- train(trip_duration_binned ~ ., data = train_data, method = "rf", trControl = train_control, tuneLength = 3)

# Predict on binned categories
rf_predictions_binned <- predict(rf_model_binned, test_data)

# Calculate accuracy
rf_accuracy <- sum(rf_predictions_binned == test_data$trip_duration_binned) / nrow(test_data)
cat("Random Forest Accuracy:", rf_accuracy, "\n")
```

```{r}
# Create a confusion matrix
conf_matrix <- confusionMatrix(rf_predictions_binned, test_data$trip_duration_binned)
print(conf_matrix)

# Plot confusion matrix
conf_matrix_plot <- as.data.frame(table(Predicted = rf_predictions_binned, Actual = test_data$trip_duration_binned))
ggplot(conf_matrix_plot, aes(x = Actual, y = Predicted, fill = Freq)) +
  geom_tile() +
  geom_text(aes(label = Freq), color = "black") +
  scale_fill_gradient(low = "white", high = "blue") +
  labs(title = "Confusion Matrix for Random Forest", x = "Actual", y = "Predicted") +
  theme_minimal()
```

```{r}
# Train Control
train_control <- trainControl(method = "cv", number = 5)

# XGBoost
set.seed(123)
xgb_model <- train(trip_duration_binned ~ ., data = train_data, method = "xgbTree", trControl = train_control, tuneLength = 5)

# Predict on the test set
xgb_predictions <- predict(xgb_model, test_data)

# Calculate RMSE and R-squared
xgb_rmse <- RMSE(as.numeric(xgb_predictions), as.numeric(test_data$trip_duration_binned))
xgb_r2 <- R2(as.numeric(xgb_predictions), as.numeric(test_data$trip_duration_binned))

# Print XGBoost performance
cat("XGBoost RMSE:", xgb_rmse, "R-squared:", xgb_r2, "\n")

# Calculate accuracy
xgb_accuracy <- sum(xgb_predictions == test_data$trip_duration_binned) / nrow(test_data)
cat("XGBoost Accuracy:", xgb_accuracy, "\n")9633
```

```{r}
# Create a data frame to store model performance
model_performance <- data.frame(
  Model = c("Random Forest", "XGBoost"),
  RMSE = c(rf_rmse, xgb_rmse),
  R2 = c(rf_r2, xgb_r2)
)

# Plot RMSE for each model
ggplot(model_performance, aes(x = Model, y = RMSE, fill = Model)) +
  geom_bar(stat = "identity") +
  labs(title = "Model RMSE Comparison", x = "Model", y = "RMSE") +
  theme_minimal()

# Plot R-squared for each model
ggplot(model_performance, aes(x = Model, y = R2, fill = Model)) +
  geom_bar(stat = "identity") +
  labs(title = "Model R-squared Comparison", x = "Model", y = "R-squared") +
  theme_minimal()
```

```{r}
# Create a confusion matrix
xgb_conf_matrix <- confusionMatrix(xgb_predictions, test_data$trip_duration_binned)
print(xgb_conf_matrix)

# Plot confusion matrix
xgb_conf_matrix_plot <- as.data.frame(table(Predicted = xgb_predictions, Actual = test_data$trip_duration_binned))
ggplot(xgb_conf_matrix_plot, aes(x = Actual, y = Predicted, fill = Freq)) +
  geom_tile() +
  geom_text(aes(label = Freq), color = "black") +
  scale_fill_gradient(low = "white", high = "blue") +
  labs(title = "Confusion Matrix for XGBoost", x = "Actual", y = "Predicted") +
  theme_minimal()
```

```{r}
# Compare Model Performance
model_comparison <- data.frame(
  Model = c("Random Forest", "XGBoost"),
  RMSE = c(rf_rmse, xgb_rmse),
  R2 = c(rf_r2, xgb_r2),
  Accuracy = c(rf_accuracy, xgb_accuracy)
)

# Print model comparison
print(model_comparison)

# Plot Model Performance
ggplot(model_comparison, aes(x = Model)) +
  geom_bar(aes(y = RMSE, fill = "RMSE"), stat = "identity", position = "dodge") +
  geom_bar(aes(y = R2 * 100, fill = "R-squared"), stat = "identity", position = "dodge") +
  geom_bar(aes(y = Accuracy * 100, fill = "Accuracy"), stat = "identity", position = "dodge") +
  labs(title = "Model Performance Comparison", y = "Metrics (%)") +
  scale_fill_manual(name = "Metrics", values = c("RMSE" = "steelblue", "R-squared" = "darkgreen", "Accuracy" = "purple")) +
  theme_minimal()
```